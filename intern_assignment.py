# -*- coding: utf-8 -*-
"""Intern Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A6qqPFVQWRqGhzlzAof9RWc-sBxuSwwW
"""

import pandas as pd
import numpy as np

df = pd.read_excel('/content/Intern Assignment.xlsx')

df

df = df.dropna(subset=["website"])
df.reset_index(drop=True, inplace=True)
print(f"Total vendors with valid URLs: {len(df)}")

df.head()

# Keywords to exclude (e.g., infrastructure, property, steel pipes)
exclude_keywords = [
    "infra", "property", "realtech", "realtor", "steel pipes",
    "steel tubes", "trading", "fabric", "construction", "housing", "rubber", "chemicals", "textiles", "logistics", "shipping",
    "energy", "solar", "renewable", "sugar equipment", "grating","automotive", "auto parts", "hardware", "tools", "fasteners",
    "bearings", "forgings", "castings","pipes", "tubes", "fittings", "plumbing", "valves", "nozzles",
    "tubular", "hollow sections","electrical", "electronics", "power transmission", "switchgear",
    "cables", "transformers", "lighting", "wires", "electrode","infracon", "infratech", "construction", "builders", "developers",
    "projects", "contractors", "real estate", "housing", "property","steel trader", "steel merchant", "steel supplier", "steel sales",
    "iron & steel", "metal mart", "metalloys", "rolling mills", "re-rolling",
    "steel corporation", "steel center", "steel industries", "steel impex"
]

# Filter out rows where company name contains excluded keywords
filtered_df = df[
    ~df["company name"].str.lower().str.contains("|".join(exclude_keywords), na=False)
]
print(f"Vendors after name filtering: {len(filtered_df)}")

import requests
from bs4 import BeautifulSoup

def scrape_about_us(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        # Prioritize "about-us" or similar sections
        about_sections = soup.find_all(["section", "div"], class_=lambda x: x and "about" in x.lower())
        if not about_sections:
            about_sections = soup.find_all(["p", "article"])  # Fallback

        text = " ".join([section.get_text(strip=True) for section in about_sections])
        return text if text else "No 'About Us' section found."
    except Exception as e:
        return f"Scraping failed: {str(e)}"

# Test the function (optional)
print(scrape_about_us("https://www.elecon.com"))

filtered_df["about_text"] = filtered_df["website"].apply(scrape_about_us)

df_f = filtered_df.copy()

df_f

# Add a column for description length
filtered_df["desc_length"] = filtered_df["about_text"].apply(lambda x: len(str(x)))
# Flag rows where description is too short or scraping failed
filtered_df["scraping_status"] = filtered_df.apply(
    lambda row: "Failed" if row["desc_length"] < 150 or row["about_text"].startswith("Scraping failed")
               else "Success",
    axis=1
)

# Get failed URLs (short descriptions or errors)
failed_urls = filtered_df[filtered_df["scraping_status"] == "Failed"]["website"].tolist()
print(f"Websites needing Selenium/Scrapy: {len(failed_urls)}")

filtered_df["scraping_status"] = filtered_df.apply(
    lambda row: "Failed" if row["desc_length"] < 150 or row["about_text"].startswith("Scraping failed")
               else "Success",
    axis=1
)
df_1 = filtered_df[filtered_df["scraping_status"] == "Failed"]
df_1.head()

!pip install playwright
!playwright install

from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import pandas as pd
from tqdm.notebook import tqdm

def extract_about_text(soup):
    selectors = ["div", "section", "p"]
    about_candidates = []

    for tag in selectors:
        about_candidates += soup.find_all(tag, id=lambda x: x and "about" in x.lower())
        about_candidates += soup.find_all(tag, class_=lambda x: x and "about" in x.lower())

    about_texts = [el.get_text(separator=" ", strip=True) for el in about_candidates if el.get_text(strip=True)]
    about_text = " ".join(about_texts)

    return about_text.strip() if len(about_text) > 100 else None

def scrape_about_us(url):
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, timeout=60000)
            page.wait_for_load_state("load")

            soup = BeautifulSoup(page.content(), "html.parser")
            about_text = extract_about_text(soup)

            # Fallback: Click on “About” link
            if not about_text:
                links = page.query_selector_all("a")
                for link in links:
                    text = link.inner_text().strip().lower()
                    if "about" in text and "contact" not in text:
                        try:
                            with page.expect_navigation():
                                link.click()
                            page.wait_for_load_state("load")
                            soup = BeautifulSoup(page.content(), "html.parser")
                            about_text = extract_about_text(soup)
                            if about_text:
                                break
                        except:
                            continue

            browser.close()
            return about_text if about_text else "[NOT FOUND]"

    except Exception as e:
        return f"[ERROR]: {str(e)}"

tqdm.pandas()
df_1['about_section'] = df_1['website'].progress_apply(scrape_about_us)

df_f = df_f.reset_index(drop=True)

# Drop entries where about_text is 'No 'About Us' section found.'
filtered_df = filtered_df[filtered_df["scraping_status"] != "Failed"]

filtered_df

df_1.drop(['about_text', 'desc_length', 'scraping_status'], axis=1, inplace=True)

df_1 = df_1[df_1["about_section"] != "[NOT FOUND]"]
df_1 = df_1[df_1['about_section'].str.len() >= 200]
df_1

df_f

filtered_df.drop(['desc_length', 'scraping_status'], axis=1, inplace=True)

df_0 = filtered_df
df_0.reset_index(drop=True, inplace=True)
df_0

df_1.drop(['about_section_length'], axis=1, inplace=True)

df_1 = df_1.rename(columns={'about_section': 'about_text'})

df_1

df_final = pd.concat([df_0, df_1], ignore_index=True)
df_final

pip install langdetect

from langdetect import detect, LangDetectException

def is_english(text):
    try:
        # Handle NaN or empty strings
        if pd.isna(text) or not str(text).strip():
            return False
        return detect(str(text)) == 'en'
    except LangDetectException:
        return False  # If language detection fails, exclude it

# Apply the function to filter English descriptions only
df = df_final[df_final["about_text"].apply(is_english)].copy()

df

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Download NLP resources (run once)
nltk.download(['stopwords', 'wordnet', 'punkt'])
nltk.download('punkt_tab') # Download the punkt_tab data

# Initialize NLP tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    """Deep cleaning of scraped text"""
    # Basic cleaning
    text = text.lower()
    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)  # Remove URLs
    text = re.sub(r'\b\d+\b', '', text)  # Remove standalone numbers
    text = re.sub(r'[^\w\s]', ' ', text)  # Replace punctuation with space

    # Advanced cleaning
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Reduce to root form
    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]

    return ' '.join(tokens)

df['description'] = df['about_text'].apply(clean_text)

"""### Scoreing / Indexing"""

keyword_weights = {
    'core_products': {
        'agitator': 3,
        'agitation system': 3,
        'mixing device':3,
        'vibratory equipment': 4,  # Phrase match
        'mixer': 2.5,
        'dynamic mixer': 4,
        'static mixer': 3
    },
    'applications': {
        'tank': 2,
        'pressure vessel': 3,
        'reactor': 3,
        'chemical processing': 2,
        'reaction vessel': 3,
        'mixing vessel': 3,
        'stirring vessel': 3,
        'agitation vessel': 3,
        'mixing tank': 3,
    },
    'capabilities': {
        'design': 2,
        'manufactur': 3,  # Catches manufacture/manufacturing
        'supply': 1.5,
        'fabricat': 2
    }
}

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity # Import cosine_similarity

# Download NLP resources (run once)
nltk.download(['stopwords', 'wordnet', 'punkt'])
nltk.download('punkt_tab') # Download the punkt_tab data

# Initialize NLP tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    """Deep cleaning of scraped text"""
    # Basic cleaning
    text = text.lower()
    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)  # Remove URLs
    text = re.sub(r'\b\d+\b', '', text)  # Remove standalone numbers
    text = re.sub(r'[^\w\s]', ' ', text)  # Replace punctuation with space

    # Advanced cleaning
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Reduce to root form
    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]

    return ' '.join(tokens)

df['description'] = df['about_text'].apply(clean_text)

def calculate_score(text):
    """Enhanced scoring with phrase matching"""
    score = 0

    # Check for multi-word phrases first
    for category in keyword_weights.values():
        for phrase, weight in category.items():
            if len(phrase.split()) > 1:  # Handle phrases
                if re.search(r'\b' + re.escape(phrase) + r'\b', text):
                    score += weight

    # Then check single words
    words = text.split()
    for category in keyword_weights.values():
        for word, weight in category.items():
            if len(word.split()) == 1 and word in words:
                score += weight

    return score

# Apply scoring
df['relevance_score'] = df['description'].apply(calculate_score)

# Advanced TF-IDF scoring (modified for phrases)
custom_vocab = list({kw for category in keyword_weights.values() for kw in category.keys()})
vectorizer = TfidfVectorizer(vocabulary=custom_vocab, ngram_range=(1, 2))
tfidf_matrix = vectorizer.fit_transform(df['description'])

query_vec = np.zeros(len(custom_vocab))
for i, term in enumerate(custom_vocab):
    for category in keyword_weights.values():
        if term in category:
            query_vec[i] += category[term]

df['tfidf_score'] = cosine_similarity(tfidf_matrix, query_vec.reshape(1, -1)).flatten()

# Final combined score
df['final_score'] = 0.7*df['tfidf_score'] + 0.3*(df['relevance_score']/df['relevance_score'].max())

# Display top vendors
result = df.sort_values('final_score', ascending=False)[['company name', 'final_score', 'description']]
print(result)

#Top 5 companies
top_5_companies = result.head(5)
top_5_companies.reset_index(drop=True, inplace=True)
top_5_companies

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Generate wordcloud from top vendor descriptions
top_vendor_text = ' '.join(df.nlargest(3, 'final_score')['description'])
wordcloud = WordCloud(width=800, height=400).generate(top_vendor_text)
plt.imshow(wordcloud)
plt.axis('off')

"""## Additional Approaches"""

import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import pandas as pd
from tqdm.notebook import tqdm

# Helper function to extract about section
def extract_about_text(soup):
    selectors = ["div", "section", "p"]
    about_candidates = []

    for tag in selectors:
        about_candidates += soup.find_all(tag, id=lambda x: x and "about" in x.lower())
        about_candidates += soup.find_all(tag, class_=lambda x: x and "about" in x.lower())

    about_texts = [el.get_text(separator=" ", strip=True) for el in about_candidates if el.get_text(strip=True)]
    about_text = " ".join(about_texts)
    return about_text.strip() if len(about_text) > 100 else None

async def scrape_about_us_async(url):
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            await page.goto(url, timeout=60000)
            await page.wait_for_load_state("load")

            # STEP 1: Initial parse
            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")
            about_text = extract_about_text(soup)

            # STEP 2: If not found, try to find and click "About" tab
            if not about_text:
                links = await page.query_selector_all("a")
                about_link_found = False

                for link in links:
                    try:
                        text = await link.inner_text()
                        if any(variant in text.lower() for variant in ["about", "about us"]):
                            href = await link.get_attribute("href")
                            if href and not href.startswith("#"):
                                await page.goto(href if "http" in href else url.rstrip("/") + "/" + href.lstrip("/"))
                            else:
                                await link.click()
                            await page.wait_for_load_state("load")
                            about_link_found = True
                            break
                    except:
                        continue

                # STEP 3: After clicking About tab or following href
                if about_link_found:
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")
                    about_text = extract_about_text(soup)

            await browser.close()
            return about_text if about_text else "[NOT FOUND]"

    except Exception as e:
        return f"[ERROR]: {str(e)}"

# Sample df
# df = pd.DataFrame({'website': ['https://example.com', ...]})

results = []

async def scrape_all(urls):
    for url in tqdm(urls):
        text = await scrape_about_us_async(url)
        results.append(text)

# Run the async job
await scrape_all(df_1['website'].tolist())
df_1['about_section'] = results

df_nf = df_1[df_1['about_section'] == '[NOT FOUND]']

df_1

import asyncio
import pandas as pd
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

df = df_nf

ABOUT_TERMS = ["about", "About", "ABOUT", "about us", "About Us", "ABOUT US", "who we are", "Who We Are", "WHO WE ARE"]

def extract_about_text(html):
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup.find_all(["section", "div", "p", "article"]):
        text = tag.get_text(separator=" ", strip=True)
        if any(term in text for term in ABOUT_TERMS) and len(text) > 100:
            return text
    return ""

async def extract_about_from_site(url, context):
    try:
        page = await context.new_page()
        await page.goto(url, timeout=20000)
        await page.wait_for_load_state('networkidle')

        # Search for "About" buttons/links and click them
        for tag in ['a', 'button', 'li', 'span', 'div']:
            elements = await page.query_selector_all(tag)
            for el in elements:
                try:
                    text = (await el.inner_text()).strip()
                    if text in ABOUT_TERMS:
                        await el.click()
                        await page.wait_for_timeout(3000)
                        content = await page.content()
                        result = extract_about_text(content)
                        if result:
                            await page.close()
                            return result
                except: continue

        # Try to navigate to about-like links (e.g. /about)
        anchors = await page.query_selector_all("a")
        for a in anchors:
            try:
                href = await a.get_attribute("href")
                if href and any(term in href for term in ["about", "About", "ABOUT"]):
                    abs_url = href if href.startswith("http") else url.rstrip("/") + "/" + href.lstrip("/")
                    await page.goto(abs_url, timeout=10000)
                    await page.wait_for_load_state('networkidle')
                    html = await page.content()
                    result = extract_about_text(html)
                    if result:
                        await page.close()
                        return result
            except: continue

        # Fallback: scrape body content
        content = await page.content()
        await page.close()
        return extract_about_text(content)

    except Exception as e:
        return f"[FAILED]: {str(e)}"

async def process_all(df):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        results = []
        for url in df["website"]:
            print(f"Processing: {url}")
            text = await extract_about_from_site(url, context)
            results.append(text)
        await browser.close()
        df["about_content"] = results
        return df

# Run this to kick off the process
updated_df = asyncio.run(process_all(df))
print(updated_df.head())

import asyncio
import pandas as pd
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

df = df_nf

ABOUT_TERMS = ["about", "About", "ABOUT", "about us", "About Us", "ABOUT US", "who we are", "Who We Are", "WHO WE ARE"]

def extract_about_text(html):
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup.find_all(["section", "div", "p", "article"]):
        text = tag.get_text(separator=" ", strip=True)
        if any(term in text for term in ABOUT_TERMS) and len(text) > 100:
            return text
    return ""

async def extract_about_from_site(url, context):
    try:
        page = await context.new_page()
        await page.goto(url, timeout=20000)
        await page.wait_for_load_state('networkidle')

        # Search for "About" buttons/links and click them
        for tag in ['a', 'button', 'li', 'span', 'div']:
            elements = await page.query_selector_all(tag)
            for el in elements:
                try:
                    text = (await el.inner_text()).strip()
                    if text in ABOUT_TERMS:
                        await el.click()
                        await page.wait_for_timeout(3000)
                        content = await page.content()
                        result = extract_about_text(content)
                        if result:
                            await page.close()
                            return result
                except: continue

        # Try to navigate to about-like links (e.g. /about)
        anchors = await page.query_selector_all("a")
        for a in anchors:
            try:
                href = await a.get_attribute("href")
                if href and any(term in href for term in ["about", "About", "ABOUT"]):
                    abs_url = href if href.startswith("http") else url.rstrip("/") + "/" + href.lstrip("/")
                    await page.goto(abs_url, timeout=10000)
                    await page.wait_for_load_state('networkidle')
                    html = await page.content()
                    result = extract_about_text(html)
                    if result:
                        await page.close()
                        return result
            except: continue

        # Fallback: scrape body content
        content = await page.content()
        await page.close()
        return extract_about_text(content)

    except Exception as e:
        return f"[FAILED]: {str(e)}"

async def process_all(df):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        results = []
        for url in df["website"]:
            print(f"Processing: {url}")
            text = await extract_about_from_site(url, context)
            results.append(text)
        await browser.close()
        df["about_content"] = results
        return df

# Instead of asyncio.run, use the following in a Jupyter Notebook environment:
# This ensures the code runs within the existing event loop
import nest_asyncio
nest_asyncio.apply()

updated_df = asyncio.run(process_all(df)) # Now this will run within the existing event loop
print(updated_df.head())

import asyncio
import pandas as pd
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import nest_asyncio

nest_asyncio.apply()

df = df_nf.copy()  # ensure you’re not modifying the original df

ABOUT_TERMS = [
    "about", "About", "ABOUT",
    "about us", "About Us", "ABOUT US",
    "who we are", "Who We Are", "WHO WE ARE"
]

def extract_about_text(html):
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup.find_all(["section", "div", "p", "article"]):
        text = tag.get_text(separator=" ", strip=True)
        if any(term in text for term in ABOUT_TERMS) and len(text) > 100:
            return text
    return ""

async def extract_about_from_site(url, context):
    page = await context.new_page()
    try:
        await page.goto(url, timeout=20000)
        await page.wait_for_load_state('networkidle')

        # First pass: click known About terms
        for tag in ['a', 'button', 'li', 'span', 'div']:
            elements = await page.query_selector_all(tag)
            for el in elements:
                try:
                    text = (await el.inner_text()).strip()
                    if text in ABOUT_TERMS:
                        await el.click()
                        await page.wait_for_timeout(3000)
                        content = await page.content()
                        result = extract_about_text(content)
                        if result:
                            return result
                except Exception as e:
                    continue  # ignore if the element isn't clickable or errors out

        # Fallback: follow anchor links with about-like hrefs
        anchors = await page.query_selector_all("a")
        for a in anchors:
            try:
                href = await a.get_attribute("href")
                if href and any(term in href for term in ["about", "About", "ABOUT"]):
                    target_url = urljoin(url, href)
                    await page.goto(target_url, timeout=10000)
                    await page.wait_for_load_state('networkidle')
                    html = await page.content()
                    result = extract_about_text(html)
                    if result:
                        return result
            except Exception as e:
                continue  # just skip broken links

        # Final fallback: try current content
        html = await page.content()
        return extract_about_text(html)

    except Exception as e:
        return f"[FAILED]: {str(e)}"

    finally:
        await page.close()  # always close page once, after everything

async def process_all(df):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        results = []
        for url in df["website"]:
            print(f"Processing: {url}")
            try:
                text = await extract_about_from_site(url, context)
                results.append(text)
            except Exception as e:
                results.append(f"[ERROR]: {str(e)}")
        await browser.close()
        df["about_content"] = results
        return df

updated_df = asyncio.run(process_all(df))
print(updated_df.head())

import asyncio
import pandas as pd
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import nest_asyncio

nest_asyncio.apply()

df = df_nf.copy()

ABOUT_TERMS = [
    "about", "About", "ABOUT",
    "about us", "About Us", "ABOUT US",
    "who we are", "Who We Are", "WHO WE ARE"
]

def extract_about_text(html):
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup.find_all(["section", "div", "p", "article"]):
        text = tag.get_text(separator=" ", strip=True)
        if any(term in text for term in ABOUT_TERMS) and len(text) > 100:
            return text
    return ""

async def extract_about_from_site(url, context, sem):
    async with sem:
        page = await context.new_page()
        try:
            await page.goto(url, timeout=20000)
            await page.wait_for_load_state('networkidle')

            for tag in ['a', 'button', 'li', 'span', 'div']:
                elements = await page.query_selector_all(tag)
                for el in elements:
                    try:
                        text = (await el.inner_text()).strip()
                        if text in ABOUT_TERMS:
                            await el.click()
                            await page.wait_for_timeout(3000)
                            content = await page.content()
                            result = extract_about_text(content)
                            if result:
                                return result
                    except: continue

            anchors = await page.query_selector_all("a")
            for a in anchors:
                try:
                    href = await a.get_attribute("href")
                    if href and any(term in href for term in ["about", "About", "ABOUT"]):
                        target_url = urljoin(url, href)
                        await page.goto(target_url, timeout=10000)
                        await page.wait_for_load_state('networkidle')
                        html = await page.content()
                        result = extract_about_text(html)
                        if result:
                            return result
                except: continue

            html = await page.content()
            return extract_about_text(html)

        except Exception as e:
            return f"[FAILED]: {str(e)}"
        finally:
            await page.close()

async def process_all(df, max_concurrency=10):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        sem = asyncio.Semaphore(max_concurrency)

        tasks = [
            extract_about_from_site(url, context, sem)
            for url in df["website"]
        ]
        results = await asyncio.gather(*tasks)
        await browser.close()
        df["about_content"] = results
        return df

updated_df = asyncio.run(process_all(df, max_concurrency=10))
print(updated_df.head())

updated_df

import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin

# Replace with your actual dataframe
df = df_nf.copy()

ABOUT_TERMS = ["about", "about us", "who we are"]
CASE_SENSITIVE_TERMS = ["About", "About Us", "Who We Are", "ABOUT", "ABOUT US", "WHO WE ARE"]

def extract_text(soup):
    texts = soup.find_all(['section', 'div', 'p', 'article'])
    for tag in texts:
        text = tag.get_text(separator=" ", strip=True)
        if any(term in text for term in CASE_SENSITIVE_TERMS) and len(text) > 100:
            return text
    return ""

def find_about_link(soup, base_url):
    for link in soup.find_all('a', href=True):
        href = link['href']
        text = link.get_text(strip=True)
        if any(term in href or term == text for term in ABOUT_TERMS + CASE_SENSITIVE_TERMS):
            return urljoin(base_url, href)
    return None

def scrape_about(url):
    try:
        r = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(r.text, 'html.parser')

        # 1. Check homepage
        text = extract_text(soup)
        if text:
            return text

        # 2. Try to find an About Us link
        about_url = find_about_link(soup, url)
        if about_url:
            r2 = requests.get(about_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
            soup2 = BeautifulSoup(r2.text, 'html.parser')
            return extract_text(soup2)

        return "[Not Found]"

    except Exception as e:
        return f"[Error] {str(e)}"

# Apply to all URLs
df["about_content"] = df["website"].apply(scrape_about)
df.head()

